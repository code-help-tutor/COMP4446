{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of COMP5046_Lab02_withSolution.ipynb","provenance":[{"file_id":"17nO1IctvDzsgMlos_rTu4MWcmWbwq8_m","timestamp":1618666596263},{"file_id":"1WskgI-VgPPfYymBkHXp3HntMuhz41Buz","timestamp":1615507784714},{"file_id":"17DgqwpQBNFlej9vHVqzmFsr4etnHKfoa","timestamp":1615442498751},{"file_id":"1ZAeZOj__wcFSKcYJatR1yjc9Bi67KhFc","timestamp":1552031451265},{"file_id":"13FLHZ7OObuYidE0CKjrPcDgIa9rBKbq4","timestamp":1551931926160},{"file_id":"10AdEgRqTFA7HnYWkL_cfOEwFZIUZsOJz","timestamp":1551699141093}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GCHPkKbuhPF6"},"source":["# Lab 02\n","\n","## Word2Vec"]},{"cell_type":"code","metadata":{"id":"GNyhgK5QTOuD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616029261817,"user_tz":-660,"elapsed":2962,"user":{"displayName":"Henry Weld","photoUrl":"","userId":"15950399648764551550"}},"outputId":"b33130a1-ab87-45be-9c28-8400e44605ac"},"source":["import pprint\n","import re\n","\n","# For parsing our XML data\n","from lxml import etree \n","\n","# For data processing\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# For implementing the word2vec family of algorithms\n","from gensim.models import Word2Vec\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bmae7urS8RHD"},"source":["### Download data from Google Drive\n","For today's lab we will download and use the TED script data from Google Drive."]},{"cell_type":"markdown","metadata":{"id":"gV7vMHSahdnf"},"source":["#### Google Drive Access Setup\n","By running the following code, it will generate a link and a field for entering the verification code.\n","\n","Click the link, which will direct to the Google Sign In page. Sign in with your own Google account by following the instructions on the page.\n","\n","Then copy the generated verification code from the page into the verification code field and press Enter "]},{"cell_type":"code","metadata":{"id":"oTSQtnPkfyzj"},"source":["!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ewAbjQzThnT5"},"source":["#### Downloading TED Scripts from Google Drive \n","Click on left side \"Files\" tab and see the file is downloaded successfully."]},{"cell_type":"code","metadata":{"id":"EVk7tjwvhl-6"},"source":["id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n","downloaded = drive.CreateFile({'id':id}) \n","downloaded.GetContentFile('ted_en-20160408.xml')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FIPpEvI4kqMV"},"source":["### Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"VYmEQgB7XoDE"},"source":["targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n","\n","# Getting contents of <content> tag from the xml file\n","target_text = etree.parse(targetXML)\n","parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n","\n","# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n","content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n","\n","# Tokenising the sentence to process it by using NLTK library\n","sent_text=sent_tokenize(content_text)\n","\n","# Removing punctuation and changing all characters to lower case\n","normalized_text = []\n","for string in sent_text:\n","     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n","     normalized_text.append(tokens)\n","\n","# Tokenising each sentence to process individual word\n","sentences=[]\n","sentences=[word_tokenize(sentence) for sentence in normalized_text]\n","\n","# Prints only 10 (tokenised) sentences\n","print(sentences[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CojV1MbhkQxK"},"source":["### Word2Vec - Continuous Bag-Of-Words (CBOW)"]},{"cell_type":"markdown","metadata":{"id":"vLq1VIZ7TDog"},"source":["For more details about gensim.models.word2vec you can refer to [API for Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)"]},{"cell_type":"code","metadata":{"id":"zW1iEee3lZC9"},"source":["# Initialize and train a word2vec model with the following parameters:\n","# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n","# size: dimensionality of the word vectors\n","# window: window size\n","# min_count: ignores all words with total frequency lower than the specified count value\n","# workers: Use specified number of worker threads to train the model (=faster training with multicore machines)\n","# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n","wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2FKp3X7pkRm6"},"source":["# The trained word vectors are stored in a KeyedVectors instance as model.wv\n","# Get the top 10 similar words to 'man' by calling most_similar() \n","# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n","\n","similar_words=wv_cbow_model.wv.most_similar(\"man\") # topn=10 by default\n","pprint.pprint(similar_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsFHg0znlPSf"},"source":["### Word2Vec - Skip Gram"]},{"cell_type":"code","metadata":{"id":"k16AowhCWUXu"},"source":["# Now we switch to a Skip Gram model by setting parameter sg=1\n","wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8UiVfr2cBtA"},"source":["similar_words=wv_sg_model.wv.most_similar(\"man\")\n","pprint.pprint(similar_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfF7YqvpppbG"},"source":["## Word2Vec vs FastText"]},{"cell_type":"markdown","metadata":{"id":"d8IV7D6VAEcr"},"source":["Word2Vec - Skip Gram cannot find similar words to \"electrofishing\" as \"electrofishing\" is not in the vocabulary."]},{"cell_type":"code","metadata":{"id":"oS9c2uWWquWG"},"source":["similar_words=wv_sg_model.wv.most_similar(\"electrofishing\")\n","pprint.pprint(similar_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5TpkScI8sA9G"},"source":["### FastText - Skip Gram"]},{"cell_type":"code","metadata":{"id":"YAqOR1Vqps6M"},"source":["from gensim.models import FastText"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqkvyiUw_DRh"},"source":["# Now we initialize and train FastText with Skip Gram architecture (sg=1)\n","ft_sg_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kv26QObJriB7"},"source":["# As we can see, FastText allows us to obtain word vectors for out-of-vocabulary words\n","result=ft_sg_model.wv.most_similar(\"electrofishing\")\n","pprint.pprint(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X0x2aQpfsFSx"},"source":["### FastText - Continuous Bag-Of-Words (CBOW)"]},{"cell_type":"code","metadata":{"id":"BUBqvqpc2sbL"},"source":["# Now we initialize and train FastText with CBOW architecture (sg=0)\n","ft_cbow_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUj1RUzM2nLA"},"source":["# Again, FastText allows us to obtain word vectors for out-of-vocabulary words\n","result=ft_cbow_model.wv.most_similar(\"electrofishing\")\n","pprint.pprint(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hjmOhmRi7Ov"},"source":["## King - Man + Woman = ?"]},{"cell_type":"markdown","metadata":{"id":"Xw7b9OSwjGm0"},"source":["Try both CBOW and Skip Gram model to calculate \"King - Man + Woman = ?\""]},{"cell_type":"code","metadata":{"id":"ovTXjSdgrw36"},"source":["# We can specify the positive/negative word list with the positive/negative parameters\n","# Top N most similar words can be specified with the topn parameter\n","result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUtbE2jwq1to"},"source":["result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n","print(result)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PWf2I4_WZpG"},"source":["result = ft_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n","print(result)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9x51rRhWZrx"},"source":["result = ft_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n","print(result)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpAd8t-wjTMA"},"source":["This is not what we expected...Probably not enough data to answer as \"Queen\"\n","\n","Let's try with bigger sized data (Google has already trained Word2Vec with Google News data) in the following section\n"]},{"cell_type":"markdown","metadata":{"id":"GMY5w8F7rElp"},"source":["## Using Pretrained word embeddings with Gensim\n","\n"]},{"cell_type":"markdown","metadata":{"id":"keivkY13L4Nz"},"source":["### 1.Download and load from Google pretrained Word2Vec binary file\n","[Link to Project](https://code.google.com/archive/p/word2vec/)"]},{"cell_type":"code","metadata":{"id":"teQvZDSirVVC"},"source":["# Download the pre-trained vectors trained on part of Google News dataset (about 100 billion words)\n","# Beware, this file is big (3.39GB) - might be long waiting! \n","id2 = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n","downloaded = drive.CreateFile({'id':id2}) \n","downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTrXl4FRMitm"},"source":["# Uncompress the downloaded file\n","!gzip -d /content/GoogleNews-vectors-negative300.bin.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4HWa8itmJOCD"},"source":["**Note: you may encounter a session crash with the pretrained word2vec code below due to out-of-memory issues. If it happens, you may start again directly from this section.**"]},{"cell_type":"code","metadata":{"id":"64e_sRJ1rhUa"},"source":["from gensim.models import KeyedVectors\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","# Load the pretrained vectors with KeyedVectors instance - might be long waiting! \n","filename = 'GoogleNews-vectors-negative300.bin'\n","gn_wv_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvMQp2-Tr3zl"},"source":["# Now we can try to calculate \"King - Man + Woman = ?\" again\n","result = gn_wv_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"12Ws7QvPMq9s"},"source":["### 2.Load a pretrained word embedding model using API\n","The following code illustrates another way of loading pretrained word embeddings with Gensim. Here we try with GloVe embedding trained on twitter data"]},{"cell_type":"code","metadata":{"id":"cvAP4nyYM_qZ"},"source":["import gensim.downloader as api\n","\n","# download the model and return as object ready for use\n","model = api.load(\"glove-twitter-25\")  \n","# The similarity() function can calculate the cosine similarity between two given words\n","print(model.similarity(\"cat\",\"dog\"))\n","# The distance() function is another way of calculating the similarity between two given words, which returns 1-cosine similarity instead\n","print(model.distance(\"cat\",\"dog\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pqLruu6247Ze"},"source":["# [Tips] Play with Colab Form Fields \n","**The Form** supports multiple types of fields, including **input fields**, **dropdown menus**. \n","\n","In Lab1 E1, we already used the input fields. Let's try more now. You can edit this section by double-clicking it. \n","\n","Let's get familiar by changing the value in each input field (on the right) and checking the changes in the code (on the left) - vice versa"]},{"cell_type":"code","metadata":{"id":"XBNvQmee5QIG"},"source":["#@title Example form fields\n","#@markdown please put description\n","\n","string = 'examples'  #@param {type: \"string\"}\n","slider_value = 111  #@param {type: \"slider\", min: 100, max: 200}\n","number = 102  #@param {type: \"number\"}\n","date = '2020-01-05'  #@param {type: \"date\"}\n","pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n","select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n","\n","\n","#print the output\n","print(\"string is\",string)\n","print('slider_value',slider_value)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UfESzUvhCZOt"},"source":["# Exercise\n","Please complete the following **two questions** E1 and E2 and and submit your **\"ipynb\" file to Canvas**. (You can download it using \"File\" > \"Download .ipynb\")."]},{"cell_type":"markdown","metadata":{"id":"c7G_msPjCgQJ"},"source":["##E1. What are the advantages of Facebook's FastText over Google's Word2Vec?\n","Please write down your answer below with a **supportive example**, using your own words. "]},{"cell_type":"code","metadata":{"cellView":"form","id":"P7IQSvnKIdTC"},"source":["#@Lab01 - E1\n","\n","Answer = \" Please refer to Lecture 2 slides (recording), page 67-70 where we discussed about limitation of Word2Vec and how FastText can deal with the limitation \" #@param {type:\"raw\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DupJY3rOcozM"},"source":["## E2. Let's find synonyms\n","Let's assume the cosine similarity, or distance, between two word embedding vectors can indicate if the words are semantically similar to each other. In this exercise, you will implement a function called find_synonym(), in which:\n","\n","1. A list of 6 words are given\n","2. You need to implement your own algorithm to find the **synonym for each of the words (i.e. words with the highest cosine similarity or smallest distance)** in the list **from the rest of 5 words** based on the cosine similarity calculated. (Using the .similarity() or distance() function from *Load pretrained word embedding model using API* section above may help)\n","3. Print out the synonyms found\n","\n","Please use the pretrained 50-dimensional GloVe word embedding trained on wikipedia and gigaword corpus. (You can use the gensim.downloader to load by passing 'glove-wiki-gigaword-50' to the .load() function, refer to the *Load pretrained word embedding model using API* section above)\n","\n","Before the function, you may need to import any required libraries.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"vUSfDt0bBrdL"},"source":["print(model.similarity(\"upset\",\"angry\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCd23jclUGYF"},"source":["# Complete the following function based on the requirements above\n","\n","# The list of words to find synonyms\n","words = [\"beautiful\", \"smart\", \"clever\", \"stupid\", \"lovely\", \"foolish\"]\n"," \n","# Load GloVe\n"," \n","def find_synonym(word):  \n"," \n","  \n","  # Find synonym and return the synonym from the given words list for the given word \n","  \n","\n","  \n"," \n","# Call the function to get the synonyms and print out the synonyms for each word\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s_dhm3zSYdFg"},"source":["# E2. Sample Solution\n","As long as your solution covers the requirements well and the output can show you got proper and expected output, then you should be able to get all the marks."]},{"cell_type":"code","metadata":{"id":"CuScrnB_WSIc"},"source":["# Complete the following function based on the requirements above\n","import gensim.downloader as api\n","import pprint\n"," \n","# The list of words to find synonym\n","words = [\"beautiful\", \"smart\", \"clever\", \"stupid\", \"lovely\", \"foolish\"]\n"," \n","# Load glove-wiki-gigaword-50 word embedding model\n","model = api.load(\"glove-wiki-gigaword-50\") \n"," \n","def find_synonym(word):  \n"," \n","  \n","  # Find synonym and return the synonym from the given words list for the given word \n","  similarities = [(other_word, model.similarity(word, other_word)) for other_word in words if other_word!=word]\n","  similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n","  return (word, similarities[0][0])\n","\n","  \n"," \n","# Call the function to get the synonyms and print out the synonyms for each word\n","words_syn=[find_synonym(word) for word in words]\n","pprint.pprint(words_syn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YrjbuZYrXD88"},"source":["# Extension"]},{"cell_type":"markdown","metadata":{"id":"8UWjBxLdTcEi"},"source":["## Word Embedding Visual Inspector (WEVI)\n","If you would like to visualise how Word2Vec is learning, the following link is useful https://ronxin.github.io/wevi/"]}]}